
require('dotenv').config();
const axios = require('axios');

// --- Configuraci√≥n CORS ---
const allowCors = (fn) => async (req, res) => {
  res.setHeader('Access-Control-Allow-Credentials', true);
  res.setHeader('Access-Control-Allow-Origin', '*');
  res.setHeader('Access-Control-Allow-Methods', 'POST, OPTIONS');
  res.setHeader('Access-Control-Allow-Headers', 'X-CSRF-Token, X-Requested-With, Accept, Accept-Version, Content-Length, Content-MD5, Content-Type, Date, X-Api-Version');
  if (req.method === 'OPTIONS') {
    res.status(200).end();
    return;
  }
  return await fn(req, res);
};

// ‚úÖ CORRECCI√ìN: Lista Maestra actualizada con los modelos m√°s recientes de Gemini 2.5.
// El orden es de mayor a menor capacidad/costo.
const ALL_MODELS = [
  'gemini-2.5-pro',         // El m√°s nuevo y potente
  'gemini-2.5-flash',       // El m√°s nuevo y r√°pido
  'gemini-1.5-pro-latest',    // Fallback potente de la generaci√≥n anterior
  'gemini-1.5-flash-latest',  // Fallback r√°pido de la generaci√≥n anterior
  'gemini-pro'                // Fallback final, el m√°s antiguo
];

/**
 * Elige din√°micamente el ORDEN de los modelos a probar.
 * @param {number} totalChars - El n√∫mero total de caracteres en la conversaci√≥n.
 * @returns {string[]} Una lista ordenada de todos los modelos a probar.
 */
const getDynamicModelList = (totalChars) => {
  // Umbral ajustado: si la conversaci√≥n tiene m√°s de 4000 caracteres, usamos Pro.
  const THRESHOLD = 4000;

  if (totalChars > THRESHOLD) {
    console.log(`ü§ñ Conversaci√≥n larga (${totalChars} chars). Priorizando Pro: ${ALL_MODELS[0]}.`);
    // El orden por defecto es ideal para prompts largos: 2.5 Pro, 1.5 Pro, etc.
    return [
        ALL_MODELS[0], // gemini-2.5-pro
        ALL_MODELS[2], // gemini-1.5-pro-latest
        ALL_MODELS[1], // gemini-2.5-flash
        ALL_MODELS[3], // gemini-1.5-flash-latest
        ALL_MODELS[4], // gemini-pro
    ];
  } else {
    console.log(`‚ö° Conversaci√≥n corta (${totalChars} chars). Priorizando Flash: ${ALL_MODELS[1]}.`);
    // Para prompts cortos, priorizamos los modelos Flash por velocidad.
    return [
        ALL_MODELS[1], // gemini-2.5-flash
        ALL_MODELS[3], // gemini-1.5-flash-latest
        ALL_MODELS[0], // gemini-2.5-pro
        ALL_MODELS[2], // gemini-1.5-pro-latest
        ALL_MODELS[4], // gemini-pro
    ];
  }
};

const delay = ms => new Promise(resolve => setTimeout(resolve, ms));

const fetchFromModels = async (messages, modelList) => {
  let lastError = null;

  for (let model of modelList) {
    // El endpoint v1beta es compatible con todos estos modelos.
    const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${process.env.GOOGLE_API_KEY}`;
    console.log(`üöÄ Probando modelo para chat: ${model}`);

    try {
      const response = await axios.post(apiUrl, {
        contents: messages,
        generationConfig: {
          temperature: 0.7,
          maxOutputTokens: 8192,
        },
        safetySettings: [
          { "category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE" },
          { "category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE" },
          { "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE" },
          { "category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE" }
        ]
      }, {
        headers: { 'Content-Type': 'application/json' },
        timeout: 45000,
      });

      const candidate = response.data?.candidates?.[0];
      const hasTextResult = candidate?.content?.parts?.some(p => p.text);
      const finishReason = candidate?.finishReason;

      if (hasTextResult && (finishReason === 'STOP' || finishReason === 'MAX_TOKENS')) {
        console.log(`‚úÖ Respuesta v√°lida de ${model}. (Raz√≥n: ${finishReason})`);
        return response.data;
      }
      
      lastError = new Error(`Respuesta vac√≠a o bloqueada de ${model} (Raz√≥n: ${finishReason || 'Desconocida'})`);
      console.warn(`‚ö†Ô∏è ${lastError.message}. Probando siguiente modelo...`);
      continue;

    } catch (error) {
      lastError = error;
      const status = error.response?.status || 500;
      const errorMessage = error.response?.data?.error?.message || error.message;

      console.warn(`‚ùå Error en ${model} [${status}]: ${errorMessage}. Probando siguiente...`);
      
      if (status === 400) {
        throw error;
      }
      
      await delay(500);
      continue;
    }
  }

  console.error('‚õî Todos los modelos en la lista fallaron.');
  throw lastError || new Error('No se pudo obtener una respuesta de ning√∫n modelo de IA.');
};

// --- Handler principal ---
const handler = async (req, res) => {
  if (req.method !== 'POST') {
    res.setHeader('Allow', ['POST']);
    return res.status(405).json({ error: 'M√©todo no permitido' });
  }

  const { messages } = req.body;
  if (!messages || !Array.isArray(messages) || messages.length === 0) {
    return res.status(400).json({ error: 'El campo "messages" es requerido y debe ser un array no vac√≠o.' });
  }

  const apiKey = process.env.GOOGLE_API_KEY;
  if (!apiKey) {
    console.error('üö® GOOGLE_API_KEY no configurada.');
    return res.status(500).json({ error: 'Error de configuraci√≥n del servidor.' });
  }

  try {
    const totalChars = messages.reduce((acc, msg) => acc + (msg.parts[0].text ? msg.parts[0].text.length : 0), 0);
    const modelList = getDynamicModelList(totalChars);

    const responseData = await fetchFromModels(messages, modelList);
    return res.status(200).json(responseData);

  } catch (error) {
    console.error('üí• Error final en el handler:', error.message);
    const statusCode = error.response?.status || 500;
    let errorMessage = error.response?.data?.error?.message || 'No se pudo obtener una respuesta de los modelos. Intenta nuevamente.';

    if (statusCode === 429) errorMessage = 'Se ha excedido la cuota de solicitudes. Espera un momento.';
    else if (statusCode === 400) errorMessage = 'Solicitud inv√°lida. Revisa el contenido, puede contener informaci√≥n sensible.';
    else if (error.code === 'ECONNABORTED') errorMessage = 'La solicitud tard√≥ demasiado en responder (Timeout).';
    
    return res.status(statusCode).json({ error: errorMessage });
  }
};

module.exports = allowCors(handler);

// ==========================================
//        EXPORTACI√ìN DE LA FUNCI√ìN
// ==========================================

// Finalmente, exportamos la funci√≥n 'handler' pero "envuelta" con el middleware 'allowCors'.
// Esto significa que antes de que se ejecute 'handler', siempre se ejecutar√° primero 'allowCors'
// para asegurarse de que los permisos CORS est√©n configurados correctamente.
// Esto es lo que Vercel (o cualquier entorno Node.js serverless) necesita para usar esta funci√≥n como un endpoint de API.
module.exports = allowCors(handler);
